% Generated by _bmad create-story workflow
# Story 1.3: implementar-comando-pipeline-ingest-orquestrador-basico

Status: ready-for-dev

## Story

As a User (CLI),
I want a `pipeline.ingest --ticker <TICKER> --source <provider>` command,
so that I can trigger an end-to-end ingest using the minimal adapter and mapper.

## Acceptance Criteria

1. Given the `yfinance` adapter and canonical mapper are available, when the user runs `pipeline.ingest --ticker PETR4.SA --source yfinance`, then the pipeline fetches raw data via adapter and normalizes via mapper.
2. The command saves provider raw CSV (see Story 1.4) and persists canonical rows to DB when persistence stories (1.6) are implemented.
3. The command returns a clear success/failure exit code and logs a `job_id` for the run.
4. CLI supports flags `--dry-run` (no writes) and `--force-refresh` (ignore cache).
5. Errors are logged to `ingest_logs` with descriptive messages and non-zero exit code on unrecoverable failures.

## Tasks / Subtasks

- [ ] Task 1: Implement CLI command entrypoint `pipeline.ingest` (Typer)
  - [ ] Subtask 1.1: Wire CLI to orchestration function `pipeline.ingest_command`
  - [ ] Subtask 1.2: Add `--ticker`, `--source`, `--dry-run`, `--force-refresh` flags and validation
- [ ] Task 2: Orchestrator implementation
  - [ ] Subtask 2.1: Call `Adapter.fetch()` for provider
  - [ ] Subtask 2.2: Call Canonical Mapper to normalize
  - [ ] Subtask 2.3: When not `--dry-run`, trigger raw save (Story 1.4) and DB persist (Story 1.6)
  - [ ] Subtask 2.4: Emit structured logs including `job_id`, timings and row counts
- [ ] Task 3: CLI integration tests (smoke)
  - [ ] Subtask 3.1: Add a mocked adapter fixture to run ingest in `--dry-run` and assert orchestration flow

## Dev Notes

- Arquitetura: Typer-based CLI; orchestration layer `pipeline` sits under `src.pipeline` (ex.: `src/pipeline/__init__.py`, `src/pipeline/ingest.py`).
- Dependências/stack: `pandas`, `sqlalchemy` (SQLite), `typer`, `pandera` (validation), `python-dotenv` (env), `pytest` for tests.
- Persistence: when persistence stories are implemented, use `dados/data.db` with `prices` table and upsert by `(ticker, date)`.
- Files written during runs: `raw/<provider>/<ticker>-<ts>.csv` and `ingest_logs` entry; recommend `raw/` under project root.
- Security: do not commit provider API keys; read from env vars (.env) and advise `.env.example`.

### Project Structure Notes

- Suggested layout:
  - src/pipeline/ingest.py  # orchestrator and CLI bindings
  - src/adapters/           # provider adapters (yfinance, alphavantage)
  - src/mappers/canonical.py
  - src/db/                 # DB layer and upsert helpers
  - raw/<provider>/         # raw CSV outputs
  - dados/data.db           # SQLite DB

### References

- Source: docs/planning-artifacts/epics.md#Story-1.3 (Epic 1 — Ingestão e Persistência)
- Sprint tracking: docs/implementation-artifacts/sprint-status.yaml

## Dev Agent Record

### Agent Model Used

GPT-5 mini

### Completion Notes List

- Story file created and populated with acceptance criteria, tasks and developer guardrails.

### File List

- docs/implementation-artifacts/1-3-implementar-comando-pipeline-ingest-orquestrador-basico.md

Issue: https://github.com/phbrgnomo/Analise-financeira-B3/issues/116
